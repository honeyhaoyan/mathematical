Sender: LSF System <lsfadmin@lo-s4-020>
Subject: Job 16322768: <python main.py 1e-4> in cluster <leonhard> Exited

Job <python main.py 1e-4> was submitted from host <lo-login-02> by user <yanhao> in cluster <leonhard> at Mon May 31 19:28:23 2021
Job was executed on host(s) <2*lo-s4-020>, in queue <gpu.4h>, as user <yanhao> in cluster <leonhard> at Mon May 31 19:28:54 2021
</cluster/home/yanhao> was used as the home directory.
</cluster/home/yanhao/mathematical/residual> was used as the working directory.
Started at Mon May 31 19:28:54 2021
Terminated at Mon May 31 19:30:27 2021
Results reported at Mon May 31 19:30:27 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python main.py 1e-4
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   155.31 sec.
    Max Memory :                                 2048 MB
    Average Memory :                             1808.60 MB
    Total Requested Memory :                     2048.00 MB
    Delta Memory :                               0.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                17
    Run time :                                   118 sec.
    Turnaround time :                            124 sec.

The output (if any) follows:

/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
[Epoch 1 Training] Loss: 0.032
/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
[Epoch 1 Evaluation] L1 Loss: 0.045 MSE Loss: 0.005 PSNR Loss: 24.819
Plotting loss ...
[Epoch 2 Training] Loss: 0.026
[Epoch 2 Evaluation] L1 Loss: 0.042 MSE Loss: 0.004 PSNR Loss: 24.121
[Epoch 3 Training] Loss: 0.024
[Epoch 3 Evaluation] L1 Loss: 0.033 MSE Loss: 0.003 PSNR Loss: 25.464
[Epoch 4 Training] Loss: 0.024
[Epoch 4 Evaluation] L1 Loss: 0.041 MSE Loss: 0.005 PSNR Loss: 24.379
[Epoch 5 Training] Loss: 0.024
[Epoch 5 Evaluation] L1 Loss: 0.034 MSE Loss: 0.003 PSNR Loss: 25.032
[Epoch 6 Training] Loss: 0.024
[Epoch 6 Evaluation] L1 Loss: 0.034 MSE Loss: 0.004 PSNR Loss: 24.816
[Epoch 7 Training] Loss: 0.022
[Epoch 7 Evaluation] L1 Loss: 0.037 MSE Loss: 0.004 PSNR Loss: 24.505
[Epoch 8 Training] Loss: 0.024
[Epoch 8 Evaluation] L1 Loss: 0.043 MSE Loss: 0.006 PSNR Loss: 22.542
[Epoch 9 Training] Loss: 0.024
[Epoch 9 Evaluation] L1 Loss: 0.050 MSE Loss: 0.006 PSNR Loss: 22.136
[Epoch 10 Training] Loss: 0.024
[Epoch 10 Evaluation] L1 Loss: 0.032 MSE Loss: 0.003 PSNR Loss: 25.413
[Epoch 11 Training] Loss: 0.022
[Epoch 11 Evaluation] L1 Loss: 0.037 MSE Loss: 0.004 PSNR Loss: 24.257
Plotting loss ...
[Epoch 12 Training] Loss: 0.025
[Epoch 12 Evaluation] L1 Loss: 0.032 MSE Loss: 0.004 PSNR Loss: 24.170
[Epoch 13 Training] Loss: 0.023
[Epoch 13 Evaluation] L1 Loss: 0.041 MSE Loss: 0.006 PSNR Loss: 25.026
[Epoch 14 Training] Loss: 0.023
[Epoch 14 Evaluation] L1 Loss: 0.029 MSE Loss: 0.003 PSNR Loss: 25.343
[Epoch 15 Training] Loss: 0.024
[Epoch 15 Evaluation] L1 Loss: 0.034 MSE Loss: 0.003 PSNR Loss: 25.335
[Epoch 16 Training] Loss: 0.023
[Epoch 16 Evaluation] L1 Loss: 0.033 MSE Loss: 0.003 PSNR Loss: 25.518
[Epoch 17 Training] Loss: 0.025
[Epoch 17 Evaluation] L1 Loss: 0.040 MSE Loss: 0.005 PSNR Loss: 23.252
[Epoch 18 Training] Loss: 0.023
[Epoch 18 Evaluation] L1 Loss: 0.037 MSE Loss: 0.004 PSNR Loss: 24.134
[Epoch 19 Training] Loss: 0.024
[Epoch 19 Evaluation] L1 Loss: 0.037 MSE Loss: 0.004 PSNR Loss: 24.519
[Epoch 20 Training] Loss: 0.023
[Epoch 20 Evaluation] L1 Loss: 0.046 MSE Loss: 0.006 PSNR Loss: 22.860
[Epoch 21 Training] Loss: 0.023
[Epoch 21 Evaluation] L1 Loss: 0.034 MSE Loss: 0.003 PSNR Loss: 25.168
Plotting loss ...
[Epoch 22 Training] Loss: 0.023
[Epoch 22 Evaluation] L1 Loss: 0.037 MSE Loss: 0.005 PSNR Loss: 23.747
[Epoch 23 Training] Loss: 0.023
[Epoch 23 Evaluation] L1 Loss: 0.041 MSE Loss: 0.005 PSNR Loss: 23.478
[Epoch 24 Training] Loss: 0.024
[Epoch 24 Evaluation] L1 Loss: 0.032 MSE Loss: 0.004 PSNR Loss: 26.072
[Epoch 25 Training] Loss: 0.022
[Epoch 25 Evaluation] L1 Loss: 0.034 MSE Loss: 0.003 PSNR Loss: 25.827
Traceback (most recent call last):
  File "main.py", line 57, in <module>
    main()
  File "main.py", line 39, in main
    trainer.train()
  File "/cluster/home/yanhao/mathematical/residual/train.py", line 44, in train
    high_res_prediction = self.model(low_res)
  File "/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/yanhao/mathematical/residual/model.py", line 41, in forward
    x = self.first_block(x)
  File "/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 399, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/cluster/home/yanhao/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 395, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt
